{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/semantic-systems/t5-information-retrieval.git"
      ],
      "metadata": {
        "id": "zmu_oa7XCBNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch path to working dir\n",
        "%cd t5-information-retrieval/question_generation-master/"
      ],
      "metadata": {
        "id": "sBIe9ItvhTfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install deps\n"
      ],
      "metadata": {
        "id": "IwYdTX5vgcWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "!python -m nltk.downloader punkt"
      ],
      "metadata": {
        "id": "Lwt9FWV6ggGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train data\n"
      ],
      "metadata": {
        "id": "tjG--zXSe-yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for training\n",
        "!python prepare_data.py --task qa --model_type t5 --dataset_path data/enron_train --qg_format highlight_qg_format --max_source_length 512 --max_target_length 32 --train_file_name enron_train_data_qg_hl_t5.pt --valid_file_name enron_valid_data_qg_hl_t5.pt"
      ],
      "metadata": {
        "id": "ze0lT-qegQky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "!python run_qg.py --model_name_or_path t5-small --model_type t5 \\\n",
        "    --tokenizer_name_or_path t5_qg_tokenizer \\\n",
        "    --output_dir t5-small-qg-hl \\\n",
        "    --train_file_path data/enron_train_data_qg_hl_t5.pt \\\n",
        "    --valid_file_path data/enron_valid_data_qg_hl_t5.pt \\\n",
        "    --per_device_train_batch_size 32 \\\n",
        "    --per_device_eval_batch_size 32 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 5 \\\n",
        "    --seed 777 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --evaluate_during_training \\\n",
        "    --logging_steps 100"
      ],
      "metadata": {
        "id": "9xY4iG03gXBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use trained model"
      ],
      "metadata": {
        "id": "Pa_ux_VqjqLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "TdfPKArWlqcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pipelines import pipeline\n",
        "# Base\n",
        "base_nlp = pipeline(\"multitask-qa-qg\")\n",
        "# Trained\n",
        "trained_nlp = pipeline(\"multitask-qa-qg\", model=\"./t5-small-qg-hl\")"
      ],
      "metadata": {
        "id": "El6xksiRieGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load all the emails"
      ],
      "metadata": {
        "id": "qTii4nI_mvG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emails = []\n",
        "\n",
        "# Filenames for emails to load from data/enron\n",
        "emails_to_load = [\"108_\", \"10_campbell\", \"153_\", \"83_\", \"10_\", \"12_\"]\n",
        "# If False, load all emails, otherwise load specified in emails_to_load\n",
        "load_specified = False\n",
        "\n",
        "file_path = 'data/enron'\n",
        "\n",
        "if load_specified:\n",
        "  for email in emails_to_load:\n",
        "    with open(file_path + email) as f:\n",
        "      contents = f.read()\n",
        "      emails.append(contents)\n",
        "else:\n",
        "  for filename in os.listdir(file_path):\n",
        "   # only files\n",
        "   if not os.path.isfile(os.path.join(file_path, filename)):\n",
        "     continue\n",
        "   with open('data/enron/' + filename, 'r') as f:\n",
        "      contents = f.read()\n",
        "      emails.append(contents)\n",
        "\n",
        "print(\"loaded all emails\")"
      ],
      "metadata": {
        "id": "7eBYKKQojxEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally use the model to work on the emails"
      ],
      "metadata": {
        "id": "qAJTeuxhmyFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_answer(model, question, context):\n",
        "  return model({\n",
        "      'question': question,\n",
        "      'context': context\n",
        "  })"
      ],
      "metadata": {
        "id": "3vUFo-z_NvLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question):\n",
        "  for email in emails:\n",
        "    base_answer = base_nlp({\n",
        "      \"question\": question,\n",
        "      \"context\": email\n",
        "      })\n",
        "    trained_answer = trained_nlp({\n",
        "      \"question\": question,\n",
        "      \"context\": email\n",
        "      })\n",
        "    print(\"Base Answer:\", base_answer)\n",
        "    print(\"Trained Answer:\", trained_answer)"
      ],
      "metadata": {
        "id": "dTpMIE6s-zxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What is the subject of the document?\")"
      ],
      "metadata": {
        "id": "jZpYNXMkm0X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"Who sent the email?\")"
      ],
      "metadata": {
        "id": "TAk8bj7omz7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What events are described in the email?\")"
      ],
      "metadata": {
        "id": "cOQ8mCLm6erb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tlq3k74z_IXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to display how well our model is doing compared to the annotated data."
      ],
      "metadata": {
        "id": "BMBK0rz3LxJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "valid_data_json = None\n",
        "with open('data/enron_train/valid_output.json', 'r') as f:\n",
        "      content = f.read()\n",
        "      valid_data_json = json.loads(content)"
      ],
      "metadata": {
        "id": "OV_vm9nyL3T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some functions that should help us to evluate the answers given by the model"
      ],
      "metadata": {
        "id": "a7Qq1BgQXTui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Use f1 score to compute the accuracy of an answer\n",
        "# Copied from https://rajpurkar.github.io/SQuAD-explorer/ -> https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
        "\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()\n",
        "\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "  gold_toks = get_tokens(a_gold)\n",
        "  pred_toks = get_tokens(a_pred)\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1"
      ],
      "metadata": {
        "id": "4ar5aroLW-Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for document in valid_data_json['data']:\n",
        "  for q_paragraph in document['paragraphs']:\n",
        "    print('Document:')\n",
        "    for question in q_paragraph['qas']:\n",
        "      print('Question:', question['question'])\n",
        "      print('Expected answers:', [ answer['text'] for answer in question['answers'] ])\n",
        "      # [0] since the model (currently only gives one answer)\n",
        "      model_answer = get_model_answer(trained_nlp, question['question'], q_paragraph['context'])[0]\n",
        "      print('Model answer:', model_answer)\n",
        "      print('f1 score for each annotated answer:', [compute_f1(model_answer, answer['text']) for answer in question['answers']])\n",
        "      print('\\n')"
      ],
      "metadata": {
        "id": "QmJejzcJMNao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "giUhXmksM97d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}