{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For this example the trained models are saved in a goole drive folder named `fine_tuned_models_data`"
      ],
      "metadata": {
        "id": "bK9je4KGUsba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve9ZaOv_WepQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/MyDrive/fine_tuned_models_data"
      ],
      "metadata": {
        "id": "21yMTUCLXHxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "BG4DiJZHXRxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "hovv7AuRuObD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "zLsh4JgjZS15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to set up the pipeline"
      ],
      "metadata": {
        "id": "Zl-hCfFFYRTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_qa_input(question, context):\n",
        "    \"\"\"\n",
        "    Generate the input for the model to answer the given question\n",
        "    :param question: Question to answer\n",
        "    :param context: Context to find answer in\n",
        "    :return: Input for the QA task\n",
        "    \"\"\"\n",
        "    return f\"question: {question}  context: {context}\""
      ],
      "metadata": {
        "id": "HavIsFa-YUf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5TokenizerFast, T5ForConditionalGeneration, StoppingCriteriaList, MaxLengthCriteria\n",
        "import torch\n",
        "\n",
        "class Pipeline:\n",
        "    \"\"\"\n",
        "    Pipleine for using a T5 model for question answering\n",
        "    \"\"\"\n",
        "    VALID_MODELS = [\"t5-small\", \"t5-base\", \"t5-large\", \"google/t5-small-ssm-nq\", \"google/t5-base-ssm-nq\",\n",
        "                    \"google/t5-large-ssm-nq\", \"google/flan-t5-small\", \"google/flan-t5-base\", \"google/flan-t5-large\",\n",
        "                    \"t5-small-ssm-nq\", \"t5-large-ssm-nq\"]\n",
        "\n",
        "    def __init__(self, model, is_fine_tuned, model_max_length=1024, use_cuda=False):\n",
        "        # if model not in self.VALID_MODELS:\n",
        "        #     raise ValueError(\"Specified model is not supported\")\n",
        "        self.model_name = model if not is_fine_tuned else f\"{model}_fine_tuned\"\n",
        "        self.tokenizer = get_tokenizer(model, model_max_length=model_max_length)\n",
        "        self.model = get_model(model, is_fine_tuned)\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def answer_question(self, question, context, num_answers=1):\n",
        "        if num_answers == 0:\n",
        "            return []\n",
        "        prompt = generate_qa_input(question, context)\n",
        "\n",
        "        features = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"longest\",\n",
        "            max_length=len(prompt),\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # contrastive search\n",
        "        # stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=len(context))])\n",
        "        # model_output = []\n",
        "        # for n in range(num_answers):\n",
        "        #   answer = self.model.generate(\n",
        "        #     input_ids=features[\"input_ids\"].to(self.device),\n",
        "        #     attention_mask=features['attention_mask'].to(self.device),\n",
        "        #     max_new_tokens=len(context),\n",
        "        #     penalty_alpha=0.6,\n",
        "        #     top_k=4,\n",
        "        #   )\n",
        "        #   model_output.append(self.tokenizer.decode(answer[0], skip_special_tokens=True))\n",
        "        # return model_output\n",
        "\n",
        "        # For multinomial sampling only:\n",
        "        # model_output = []\n",
        "        # for n in range(num_answers):\n",
        "        #   answer = self.model.generate(\n",
        "        #     input_ids=features[\"input_ids\"].to(self.device),\n",
        "        #     attention_mask=features['attention_mask'].to(self.device),\n",
        "        #     max_new_tokens=len(context),\n",
        "        #     num_beams=1,\n",
        "        #     do_sample=True,\n",
        "        #   )\n",
        "        #   model_output.append(self.tokenizer.decode(answer[0], skip_special_tokens=True))\n",
        "        # return model_output\n",
        "\n",
        "        # Beam search variants:\n",
        "        # model_output = self.model.generate(\n",
        "        #     input_ids=features[\"input_ids\"].to(self.device),\n",
        "        #     attention_mask=features['attention_mask'].to(self.device),\n",
        "        #     max_new_tokens=len(context),\n",
        "        #     num_beams=num_answers * 4,\n",
        "        #     # num_beam_groups=num_answers * 2,\n",
        "        #     do_sample=True,\n",
        "        #     num_return_sequences=num_answers\n",
        "        # )\n",
        "\n",
        "        # Default:\n",
        "        model_output = self.model.generate(\n",
        "            input_ids=features[\"input_ids\"].to(self.device),\n",
        "            attention_mask=features['attention_mask'].to(self.device),\n",
        "            max_new_tokens=len(context),\n",
        "            num_beams=num_answers,\n",
        "            num_return_sequences=num_answers\n",
        "        )\n",
        "\n",
        "        return [self.tokenizer.decode(out, skip_special_tokens=True) for out in model_output]\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"{type(self).__name__}(model={self.model_name})\"\n",
        "\n",
        "\n",
        "def get_tokenizer(model, model_max_length=512):\n",
        "    if \"ssm-nq\" in model:\n",
        "      return T5TokenizerFast.from_pretrained(\"google/t5-large-ssm-nq\", model_max_length=model_max_length)\n",
        "    elif \"flan-t5\" in model:\n",
        "      return T5TokenizerFast.from_pretrained(\"google/flan-t5-base\", model_max_length=model_max_length)\n",
        "    return T5TokenizerFast.from_pretrained(model, model_max_length=model_max_length)\n",
        "\n",
        "\n",
        "def get_model(model, is_fine_tuned):\n",
        "    if is_fine_tuned:\n",
        "        print(\"Loading fine tuned model from\", f\"./trained models/{model}-trained\")\n",
        "        return T5ForConditionalGeneration.from_pretrained(f\"./trained models/{model}-trained\")\n",
        "    return T5ForConditionalGeneration.from_pretrained(model)"
      ],
      "metadata": {
        "id": "9pjQD0TaXdCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enum for categorizing questions\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "class QuestionType(str, Enum):\n",
        "    is_mentioned = \"Who is mentioned?\"\n",
        "    is_attached = \"What is attached to the email?\"\n",
        "    is_requested = \"What is requested?\"\n",
        "    requested_who = \"Who is the requester?\"\n",
        "    subject = \"What is the subject of the email?\"\n",
        "    receiver_who = \"Who received the email?\"\n",
        "    receiver_main = \"Who is the main recipient?\"\n",
        "    sender_who = \"Who sent the email?\"\n",
        "    is_event = \"What event is described?\"\n",
        "    event_when = \"When does the event take place?\"\n",
        "    event_where = \"Where does the event take place?\"\n",
        "    email_address_of = \"What is the email address of x?\"\n",
        "    phone_number_of = \"What is the phone number of x?\"\n",
        "    who_is = \"Who is x?\"\n",
        "    other = \"Other\"\n",
        "\n",
        "\n",
        "# Take a question string and return the corresponding enum object if it exists\n",
        "def categorize_question(question):\n",
        "    try:\n",
        "        return QuestionType(question)\n",
        "    except ValueError:\n",
        "        if question.startswith(\"What is the email address of\"):\n",
        "            return QuestionType.email_address_of\n",
        "        elif question.startswith(\"What is the phone number of\") or question.startswith(\"What are the phone numbers\"):\n",
        "            return QuestionType.phone_number_of\n",
        "        elif question.startswith(\"Who is\"):\n",
        "            return QuestionType.who_is\n",
        "        elif \"When\" in question and any(string in question for string in [\"take place\", \"event\"]) or question.startswith(\"When was\"):\n",
        "            return QuestionType.event_when\n",
        "        elif \"Where\" in question and \"take place\" in question:\n",
        "            return QuestionType.event_where\n",
        "        # Might remove the following branches later as those wordings are no longer being used\n",
        "        elif question == \"Who also received the email?\":\n",
        "            return QuestionType.receiver_who\n",
        "        elif question == \"What entities are mentioned?\" or question == \"What is mentioned?\":\n",
        "            return QuestionType.is_mentioned\n",
        "        elif question.startswith(\"Who requested\"):\n",
        "            return QuestionType.requested_who\n",
        "        else:\n",
        "            print(\"Uncategorized question:\", question)\n",
        "            return QuestionType.other"
      ],
      "metadata": {
        "id": "46kiYo3uhlsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"./\"\n",
        "DATASET_NAME = \"valid_output_preprocessed\"\n",
        "MODEL_TO_USE = \"t5-large\"\n",
        "MODEL_FILE_NAME = MODEL_TO_USE\n",
        "IS_FINE_TUNED = True\n",
        "WITH_NONE = False\n",
        "USE_CUDA = True"
      ],
      "metadata": {
        "id": "hdWlZUzGe2SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def main():\n",
        "    print(f\"Starting answering pipeline for model {MODEL_TO_USE} (fine_tuned={IS_FINE_TUNED}, with_none={WITH_NONE})\")\n",
        "    model = Pipeline(MODEL_TO_USE, is_fine_tuned=IS_FINE_TUNED, use_cuda=USE_CUDA, model_max_length=4096)\n",
        "    answers = answer_questions(DATASET_PATH + DATASET_NAME + \".json\", model, with_none=WITH_NONE)\n",
        "    file_ending = \"_without_\" if not WITH_NONE else \"_with_\"\n",
        "    file_ending += \"none.json\"\n",
        "    file_name = DATASET_NAME + \"_\" + MODEL_FILE_NAME + f\"_fine_tuned_model_answers{file_ending}\" if IS_FINE_TUNED \\\n",
        "        else DATASET_NAME + \"_\" + MODEL_FILE_NAME + f\"_model_answers{file_ending}\"\n",
        "    # TODO: Ersetzen\n",
        "    # with open(file_name, \"w\") as file:\n",
        "    with open(FILE, \"w\") as file:\n",
        "        file.write(json.dumps(answers, indent=2))\n",
        "    # print(json.dumps(answers, indent=2))\n",
        "\n",
        "\n",
        "def answer_questions(path, model, doc_limit=0, with_none=True):\n",
        "    \"\"\"\n",
        "    :param path: Path to the qa data\n",
        "    :param model: model to use for answer\n",
        "    :param doc_limit: If greater than 0: Limit of documents to load\n",
        "    :param with_none: Ask the model for an answer if none is present?\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    qas = load_qas(path, doc_limit) if doc_limit > 0 else load_qas(path)\n",
        "    for count, context in enumerate(qas):\n",
        "        for question in context[\"questions\"]:\n",
        "            # print(f\"Answering question: {question}\")\n",
        "            # If the question has no answers, append a None answer for later use\n",
        "            if with_none:\n",
        "                if len(question[\"answers\"]) == 0:\n",
        "                    question[\"answers\"].append({\n",
        "                        \"answer_start\": 0,\n",
        "                        \"text\": \"None\",\n",
        "                    })\n",
        "            question[\"model_answers\"] = model.answer_question(question[\"question\"], context[\"context\"],\n",
        "                                                              num_answers=len(question[\"answers\"]))\n",
        "        print(f\"Answered {count + 1} of {len(qas)} documents\")\n",
        "    return qas\n",
        "\n",
        "\n",
        "def load_qas(path, limit=999999):\n",
        "    \"\"\"\n",
        "    :param path:\n",
        "    :param limit:\n",
        "    :return: {\n",
        "        context: String,\n",
        "        questions: [{\n",
        "                question: String,\n",
        "                answers: [String]\n",
        "            }]\n",
        "    }\n",
        "    \"\"\"\n",
        "    qas = []\n",
        "    with open(path) as file:\n",
        "        json_file = json.load(file)\n",
        "        for paragraph in json_file[\"data\"][:limit]:\n",
        "            for qa in paragraph[\"paragraphs\"]:\n",
        "                context = {\n",
        "                    \"context\": qa[\"context\"],\n",
        "                    \"questions\": []\n",
        "                }\n",
        "                for question in qa[\"qas\"]:\n",
        "                    question_text = question[\"question\"]\n",
        "                    answers = []\n",
        "                    for answer in question[\"answers\"]:\n",
        "                        answers.append(answer)\n",
        "                    context[\"questions\"].append({\n",
        "                        \"question\": question_text,\n",
        "                        \"question_category\": categorize_question(question_text),\n",
        "                        \"answers\": answers\n",
        "                    })\n",
        "                qas.append(context)\n",
        "    return qas"
      ],
      "metadata": {
        "id": "v7hXTCsoXtpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_TO_USE = \"t5-small\"\n",
        "MODEL_FILE_NAME = MODEL_TO_USE\n",
        "IS_FINE_TUNED = True\n",
        "WITH_NONE = True\n",
        "FILE = \"egal\""
      ],
      "metadata": {
        "id": "2MVYyNXJHjZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "main()\n",
        "end = time.time()\n",
        "print(\"finished execution, total time elapsed in seconds:\", end - start)"
      ],
      "metadata": {
        "id": "JiESsUbYYu3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AWjBOEP9ZBJ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}